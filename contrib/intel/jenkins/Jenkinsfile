import groovy.transform.Field

properties([disableConcurrentBuilds(abortPrevious: true)])
@Field def DO_RUN=true
@Field def TARGET="main"
@Field def SCRIPT_LOCATION="upstream/libfabric/contrib/intel/jenkins"
@Field def RELEASE=false
@Field def BUILD_MODES=["reg", "dbg", "dl"]
@Field def PYTHON_VERSION="3.9"
@Field def TIMEOUT="7200"

def run_python(version, command, output=null) {
  if (output != null)
    sh "python$version $command >> $output"
  else
    sh "python$version $command"
}

def slurm_batch(partition, node_num, output, command) {
  try {
    sh """sbatch --partition=${partition} -N ${node_num} \
          --wait -o ${output} --open-mode=append \
          --wrap=\'env; timeout $TIMEOUT ${command}\'
       """
  } catch (Exception e) {
    sh "scancel \$(cat ${output} | grep SLURM_JOBID | cut -d \"=\" -f 2)"
    sh "cat ${output}"
    error("Build failed ${e}")
  }
  sh "cat ${output}"
}

def run_fabtests(stage_name, hw, partition, node_num, prov, util=null,
                 user_env=null, way=null) {
  def command = "python3.9 ${RUN_LOCATION}/runtests.py --build_hw=${hw}"
  def opts = "--prov=${prov} --test=fabtests"
  def modes = BUILD_MODES
  if (util)
    opts = "${opts} --util=${util}"

  if (user_env)
    opts = "${opts} --user_env ${user_env}"

  if (way) {
    opts = "${opts} --way ${way}"
    stage_name = "${stage_name}_${way}"
    modes = ["reg"]
  }

  for (mode in modes) {
    echo "Running $stage_name fabtests $mode"
    slurm_batch("${partition}", "${node_num}",
                "${env.LOG_DIR}/${stage_name}_fabtests_${mode}",
                "${command} ${opts} --ofi_build_mode=${mode}")
  }

  echo "${stage_name} completed."
}

def run_middleware(providers, stage_name, test, hw, partition, node_num,
                   mpi=null, imb_grp=null) {
  def base_cmd = "python3.9 ${RUN_LOCATION}/runtests.py --test=${test} --build_hw=${hw}"
  def opts = ""
  def prefix = "${env.LOG_DIR}/${stage_name}_"
  def suffix = "_${test}_reg"
  if (mpi) {
    base_cmd = "${base_cmd} --mpi=${mpi}"
    suffix = "_${mpi}${suffix}"
  }

  if (imb_grp)
    base_cmd = "${base_cmd} --imb_grp=${imb_grp}"

  if (env.WEEKLY.toBoolean())
    base_cmd = "${base_cmd} --weekly=${env.WEEKLY}"

  for (prov in providers) {
    if (prov[1]) {
      echo "Running ${prov[0]}-${prov[1]} ${stage_name}"
      opts = "--prov=${prov[0]} --util=${prov[1]}"
      output = "${prefix}${prov[0]}-${prov[1]}${suffix}"
    } else {
      echo "Running ${prov[0]} ${stage_name}"
      opts = "--prov=${prov[0]}"
      output = "${prefix}${prov[0]}${suffix}"
    }

    slurm_batch("${partition}", "${node_num}", "${output}",
                "${base_cmd} ${opts}")
  }
}

def gather_logs(cluster, key, dest, source) {
  def address = "${env.USER}@${cluster}"

  try {
    sh "scp -i ${key} ${address}:${source}/* ${dest}/"
  } catch (Exception e) {
    echo "Caught exception ${e} when transfering files from ${cluster}"
  }
}

def summarize(item, verbose=false, release=false, send_mail=false) {
  def cmd = "${RUN_LOCATION}/summary.py --summary_item=all"
  if (verbose) {
    cmd = "${cmd} -v "
  }
  if (release) {
    cmd = "${cmd} --release "
  }
  if (send_mail.toBoolean()) {
    cmd = "${cmd} --send_mail "
  }

  run_python(PYTHON_VERSION, cmd)
}

def save_summary() {
  sh """
    mkdir -p ${env.WORKSPACE}/internal
    rm -rf ${env.WORKSPACE}/internal/*
    git clone https://${env.PAT}@github.com/${env.INTERNAL} ${env.WORKSPACE}/internal
    cd ${env.WORKSPACE}/internal
    mkdir -p ${env.WORKSPACE}/internal/summaries
    cp ${env.WORKSPACE}/summary_*.log ${env.WORKSPACE}/internal/summaries/
    git add ${env.WORKSPACE}/internal/summaries/
    git commit -am \"add ${env.JOB_NAME}'s summary\"
    git pull -r origin master
    git push origin master
  """
}

def checkout_upstream() {
  def loc = "${env.WORKSPACE}/upstream/libfabric"
  sh """
    if [[ ! -d ${env.WORKSPACE}/upstream ]]; then
      mkdir -p ${loc}
    else
      rm -rf ${env.WORKSPACE}/upstream && mkdir -p ${loc}
    fi

    git clone --branch ${TARGET} ${env.UPSTREAM} ${loc}
  """
}

def checkout_ci_resources() {
  sh """
    if [[ ! -d ${env.WORKSPACE}/upstream ]]; then
      mkdir ${env.WORKSPACE}/ci_resources
    else
      rm -rf ${env.WORKSPACE}/ci_resources && mkdir ${env.WORKSPACE}/ci_resources
    fi

    git clone ${env.CI_RESOURCES} ${env.WORKSPACE}/ci_resources

  """
}

def checkout_external_resources() {
  checkout_ci_resources()
  checkout_upstream()
}

def generate_diff(def branch_name, def output_loc) {
  sh """
    git remote add mainRepo ${env.UPSTREAM}
    git fetch mainRepo
    git diff --name-only HEAD..mainRepo/${branch_name} > ${output_loc}/commit_id
    git remote remove mainRepo
  """
}

def generate_release_num(def branch_name, def output_loc) {
  sh """
    git remote add mainRepo ${env.UPSTREAM}
    git fetch mainRepo
    git diff mainRepo/${branch_name}:Makefile.am Makefile.am > \
        ${output_loc}/Makefile.am.diff
    git diff mainRepo/${branch_name}:configure.ac configure.ac > \
        ${output_loc}/configure.ac.diff
    cat configure.ac | grep AC_INIT | cut -d ' ' -f 2 | \
        cut -d '[' -f 2 | cut -d ']' -f 1 > ${output_loc}/release_num.txt
    git remote remove mainRepo
  """
}

def slurm_build(modes, partition, location, hw=null, additional_args=null) {
  def cmd = "pwd; "
  def prefix = "python${PYTHON_VERSION} ${RUN_LOCATION}/build.py"
  def libfabric = "--build_item=libfabric --build_loc=${CUSTOM_WORKSPACE}/${location}/libfabric"
  def fabtests = "--build_item=fabtests --build_loc=${CUSTOM_WORKSPACE}/${location}/libfabric/fabtests"
  if (RELEASE) {
    prefix = "${prefix} --release"
  }

  if (hw) {
    prefix = "${prefix} --build_hw=${hw}"
  }

  if (additional_args) {
    prefix = "${prefix} ${additional_args} "
  }

  for (mode in modes) {
    cmd = "${cmd} ${prefix} ${libfabric} --ofi_build_mode=${mode};"
    cmd = "${cmd} ${prefix} ${fabtests} --ofi_build_mode=${mode};"
  }

  slurm_batch(partition, "1", "${env.LOG_DIR}/libfabric_${partition}", cmd)
}

def build(item, mode=null, hw=null, additional_args=null) {
  def cmd = "${RUN_LOCATION}/build.py --build_item=${item}"

  if (item == "fabtests") {
    cmd ="${cmd} --build_loc=${CUSTOM_WORKSPACE}/source/libfabric/fabtests"
  } else {
    cmd ="${cmd} --build_loc=${CUSTOM_WORKSPACE}/source/libfabric"
  }

  if (mode) {
    cmd = "${cmd} --ofi_build_mode=${mode} "
  }

  if (hw) {
    cmd = "${cmd} --build_hw=${hw} "
  }

  if (RELEASE) {
    cmd = "${cmd} --release "
  }

  if (additional_args) {
    cmd = "${cmd} ${additional_args} "
  }

  run_python(PYTHON_VERSION, cmd)
}

def check_target() {
  echo "CHANGE_TARGET = ${env.CHANGE_TARGET}"
  if (changeRequest()) {
    TARGET = env.CHANGE_TARGET
  }

  if (TARGET) {
    return TARGET
  }

  return "main"
}

def release() {
  def file = "${env.WORKSPACE}/commit_id"
  if (!fileExists(file)) {
    echo "CI Run has not rebased with ofiwg/libfabric. Please Rebase."
    return 1
  }

  def changes = readFile file
  def changeStrings = new ArrayList<String>()

  for (line in changes.readLines()) {
    changeStrings.add(line)
  }

  if ((changeStrings.toArray().any { it =~ /(Makefile\.am)\b/ }) ||
      (changeStrings.toArray().any { it =~ /(configure\.ac)\b/ })) {
        echo "This is probably a release"
        return true
  }

  return false
}

pipeline {
  agent {
    node {
      label 'main'
      customWorkspace "workspace/${JOB_NAME}/${env.BUILD_NUMBER}"
    }
  }
  options {
      timestamps()
      timeout(activity: true, time: 6, unit: 'HOURS')
      skipDefaultCheckout()
  }
  environment {
      JOB_CADENCE = 'PR'
      WITH_ENV="'PATH+EXTRA=/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/bin:$PYTHONPATH'"
      RUN_LOCATION="${env.WORKSPACE}/${SCRIPT_LOCATION}/"
      CUSTOM_WORKSPACE="${CB_HOME}/workspace/${JOB_NAME}/${env.BUILD_NUMBER}"
      LOG_DIR = "${env.CUSTOM_WORKSPACE}/log_dir"
      WEEKLY = 0
  }
  stages {
    stage ('checkout') {
      steps {
        script {
          dir ("${CUSTOM_WORKSPACE}/source/libfabric") {
            checkout scm
          }
          dir ("${CUSTOM_WORKSPACE}/grass/libfabric") {
            checkout scm
          }
          dir ("${CUSTOM_WORKSPACE}/water/libfabric") {
            checkout scm
          }
          dir ("${CUSTOM_WORKSPACE}/electric/libfabric") {
            checkout scm
          }
          dir (CUSTOM_WORKSPACE) {
            checkout_external_resources()
          }
        }
      }
    }
    stage ('prepare build') {
      when { equals expected: true, actual: DO_RUN }
      steps {
        script {
          echo "Copying build dirs."
          build("builddir")
          echo "Copying log dirs."
          build("logdir")
        }
      }
    }
    stage ('parallel-builds') {
      when { equals expected: true, actual: DO_RUN }
      parallel {
        stage ('build-water') {
          steps {
            script {
              slurm_build(BUILD_MODES, "water", "water", "water")
              slurm_batch("squirtle,totodile", "1",
                            "${env.LOG_DIR}/build_mpich_water_log",
                            """python$PYTHON_VERSION ${RUN_LOCATION}/build.py \
                              --build_item=mpich --build_hw=water"""
                          )
            }
          }
        }
        stage ('build-daos') {
          agent {
            node {
              label 'daos_head'
              customWorkspace CUSTOM_WORKSPACE
            }
          }
          options { skipDefaultCheckout() }
          steps {
            script {
              dir ("${CUSTOM_WORKSPACE}/source/libfabric") { checkout scm }
              checkout_external_resources()
              dir (CUSTOM_WORKSPACE) {
                build("logdir")
                build("libfabric", "reg", "daos")
                build("fabtests", "reg", "daos")
              }
            }
          }
        }
      }
    }
    stage('parallel-tests') {
      when { equals expected: true, actual: DO_RUN }
      parallel {
        stage('MPI_verbs-rxm_IMB') {
          steps {
            script {
              dir (RUN_LOCATION) {
                def providers = [["verbs", "rxm"]]
                for (def mpi in ["impi"]) {
                  for (imb_grp = 1; imb_grp < 4; imb_grp++) {
                    run_middleware(providers, "MPI", "IMB", "water",
                                   "squirtle,totodile", "2", "${mpi}",
                                   "${imb_grp}")
                  }
                }
              }
            }
          }
        }
        stage('MPI_verbs-rxm_OSU') {
          steps {
            script {
              dir (RUN_LOCATION) {
                def providers = [["verbs", "rxm"]]
                for (def mpi in ["impi", "mpich"]) {
                  run_middleware(providers, "MPI", "osu", "water",
                                 "squirtle,totodile", "2", "${mpi}")
                }
              }
            }
          }
        }
        stage('verbs-rxm') {
          steps {
            script {
              dir (RUN_LOCATION) {
                run_fabtests("verbs-rxm", "water", "squirtle,totodile", "2",
                             "verbs", "rxm")
                run_fabtests("verbs-rxm", "water", "squirtle,totodile", "2",
                             "verbs", "rxm", "FI_MR_CACHE_MAX_COUNT=0")
                run_fabtests("verbs-rxm", "water", "squirtle,totodile", "2",
                             "verbs", "rxm", "FI_MR_CACHE_MONITOR=userfaultfd")
              }
            }
          }
        }
        stage('mpichtestsuite') {
          steps {
            script {
              dir (RUN_LOCATION) {
                def providers = [["verbs","rxm"]]
                def MPIS = ["mpich"]
                if (env.WEEKLY.toBoolean()) {
                  MPIS = ["impi", "mpich"]
                }
                for (def mpi in MPIS) {
                  run_middleware(providers, "mpichtestsuite", "mpichtestsuite",
                                 "water", "squirtle,totodile", "2", "${mpi}")
                }
              }
            }
          }
        }
         stage('daos_verbs') {
          agent { node { label 'daos_verbs' } }
          options { skipDefaultCheckout() }
          steps {
            script {
              dir (RUN_LOCATION) {
                run_python(PYTHON_VERSION,
                           """runtests.py --prov='verbs' --util='rxm' \
                           --test=daos --build_hw=daos \
                           --log_file=${env.LOG_DIR}/daos_verbs-rxm_reg""")
              }
            }
          }
        }
      }
    }

    stage('parallel-tests-peer') {
      environment {
          FI_OFI_RXM_ENABLE_SHM=1
      }
      when { equals expected: true, actual: DO_RUN }
      parallel {
        stage('MPI_verbs-rxm_IMB-peer') {
          steps {
            script {
              dir (RUN_LOCATION) {
                def providers = [["verbs", "rxm"]]
                for (def mpi in ["impi"]) {
                  for (imb_grp = 1; imb_grp < 4; imb_grp++) {
                    run_middleware(providers, "MPI", "IMB", "water",
                                   "squirtle,totodile", "2", "${mpi}",
                                   "${imb_grp}")
                  }
                }
              }
            }
          }
        }
        stage('MPI_verbs-rxm_OSU-peer') {
          steps {
            script {
              dir (RUN_LOCATION) {
                def providers = [["verbs", "rxm"]]
                for (def mpi in ["impi", "mpich"]) {
                  run_middleware(providers, "MPI", "osu", "water",
                                 "squirtle,totodile", "2", "${mpi}")
                }
              }
            }
          }
        }
        stage('verbs-rxm-peer') {
          steps {
            script {
              dir (RUN_LOCATION) {
                run_fabtests("verbs-rxm", "water", "squirtle,totodile", "2",
                             "verbs", "rxm")
                run_fabtests("verbs-rxm", "water", "squirtle,totodile", "2",
                             "verbs", "rxm", "FI_MR_CACHE_MAX_COUNT=0")
                run_fabtests("verbs-rxm", "water", "squirtle,totodile", "2",
                             "verbs", "rxm", "FI_MR_CACHE_MONITOR=userfaultfd")
              }
            }
          }
        }
        stage('mpichtestsuite-peer') {
          steps {
            script {
              dir (RUN_LOCATION) {
                def providers = [["verbs","rxm"]]
                def MPIS = ["mpich"]
                if (env.WEEKLY.toBoolean()) {
                  MPIS = ["impi", "mpich"]
                }
                for (def mpi in MPIS) {
                  run_middleware(providers, "mpichtestsuite", "mpichtestsuite",
                                 "water", "squirtle,totodile", "2", "${mpi}")
                }
              }
            }
          }
        }
         stage('daos_verbs-peer') {
          agent { node { label 'daos_verbs' } }
          options { skipDefaultCheckout() }
          steps {
            script {
              dir (RUN_LOCATION) {
                run_python(PYTHON_VERSION,
                           """runtests.py --prov='verbs' --util='rxm' \
                           --test=daos --build_hw=daos \
                           --log_file=${env.LOG_DIR}/daos_verbs-rxm_reg""")
              }
            }
          }
        }
      }
    }
    stage ('Summary') {
      when { equals expected: true, actual: DO_RUN }
      steps {
        script {
          gather_logs("${env.DAOS_ADDR}", "${env.DAOS_KEY}", "${env.LOG_DIR}",
                      "${env.LOG_DIR}")
          gather_logs("${env.ZE_ADDR}", "${env.ZE_KEY}", "${env.LOG_DIR}",
                      "${env.LOG_DIR}")

          summarize("all", verbose=false, release=RELEASE,
                    send_mail=env.WEEKLY.toBoolean())
          if (RELEASE) {
            save_summary()
          }
        }
      }
    }
  }

  post {
    always {
      script {
        summarize("all")
      }
    }
    success {
      script {
        summarize("all", verbose=true, release=false,
        send_mail=env.WEEKLY.toBoolean())
      }
    }
    aborted {
      node ('daos_head') {
        dir ("${DELETE_LOCATION}/middlewares") { deleteDir() }
      }
      node ('ze') {
        dir ("${DELETE_LOCATION}/middlewares") { deleteDir() }
      }
      dir ("${DELETE_LOCATION}/middlewares") { deleteDir() }
    }
    cleanup {
      node ('daos_head') {
        dir("${env.WORKSPACE}") { deleteDir() }
        dir("${env.WORKSPACE}@tmp") { deleteDir() }
      }
      node ('ze') {
        dir("${env.WORKSPACE}") { deleteDir() }
        dir("${env.WORKSPACE}@tmp") { deleteDir() }
      }
      dir("${env.WORKSPACE}") { deleteDir() }
      dir("${env.WORKSPACE}@tmp") { deleteDir() }
    }
  }
}